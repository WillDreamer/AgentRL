# https://us-east-2.console.aws.amazon.com/bedrock/home?region=us-east-2#/model-catalog
import json
import os
import openai
import boto3
import argparse
import time

class ModelBase:

    def respond(self, messages: list, max_tokens: int,
                max_context_size: int) -> str:
        '''
        Generate a response for the given messages.
        Inputs:
        `messages`:
            Model-specific context messages. This is the output of the
            `self.compose_context` method.
        `max_tokens`:
            Maximum number of tokens to generate in the response.

        Output:
        A string response generated by the model.
        '''
        raise NotImplementedError()


def process_response(response):
    '''
    把不同平台返回的 异构 JSON 解析成纯文本
    '''
    response = json.loads(response.get('body').read())
    print(response)
    outputs = ""
    if "content" in response:
        content = response["content"]
        outputs = "\n\n".join([content[i]["text"] for i in range(len(content))])
    elif "generation" in response:
        generation = response["generation"]
        if generation.startswith("<|startoftext|>"):
            prefix = "<|startoftext|>"
            generation = generation[len("<|startoftext|>"):]
        if generation.startswith("<|start_header_id|>"):
            prefix = "<|start_header_id|>assistant<|end_header_id|>"
        offset = len(prefix)
        outputs = generation[offset:].lstrip()
    elif "choices" in response:
        choices = response["choices"]
        if "text" in choices[0]:
            outputs = "\n\n".join([choices[i]["text"] for i in range(len(choices))])
        elif "message" in choices[0]:
            outputs = "\n\n".join([choices[i]["message"]["content"] for i in range(len(choices))])
        else:
            raise NotImplementedError
    elif "outputs" in response:
        outputs = response["outputs"]
        outputs = "\n\n".join([outputs[i]["text"] for i in range(len(outputs))]).lstrip()
    else:
        raise NotImplementedError
    print(outputs)
    return outputs

class BedRockModel(ModelBase):
    def __init__(self, model_name=None, region="us-east-1"):
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html
        # ada credentials update --provider=conduit --account=684288478426 --role=NileTeamDeveloperConduitRole --once --profile=default; aws bedrock list-foundation-models
        self.model_name = model_name
        self.bedrock = boto3.client('bedrock-runtime', region_name=region)

    def respond(self, messages, max_tokens, max_context_size):
        raise NotImplementedError


class AnthropicModel(BedRockModel):
    model_name_list = [
        "anthropic.claude-3-haiku-20240307-v1:0",
        "anthropic.claude-3-opus-20240229-v1:0",
        "anthropic.claude-3-sonnet-20240229-v1:0",
        "anthropic.claude-3-5-haiku-20241022-v1:0",
        "us.anthropic.claude-3-5-haiku-20241022-v1:0",
        "anthropic.claude-3-5-sonnet-20241022-v2:0",
        "anthropic.claude-3-5-sonnet-20240620-v1:0",
    ]
    def __init__(self, model_name="anthropic.claude-3-haiku-20240307-v1:0",
                 region="us-east-1"):
        assert model_name.startswith("anthropic.") or model_name.startswith("us.anthropic.")
        super().__init__(model_name, region)

    def respond(self, messages, max_tokens, max_context_size):
        assert isinstance(messages, list)
        payload = {
            "messages": messages,
            "max_tokens": max_tokens,
            "anthropic_version": "bedrock-2023-05-31"
        }
        retry = 8
        while retry > 0:
            try:
                response = self.bedrock.invoke_model(body=json.dumps(payload),
                                                    modelId=self.model_name)
                return process_response(response)
            except:
                retry -= 1
                time.sleep(50/(retry+1))
                if retry == 0:
                    return None


class DeepseekModel(BedRockModel):
    model_name_list = [
        "deepseek.r1-v1:0",
        "us.deepseek.r1-v1:0",
    ]
    def __init__(self, model_name="deepseek.r1-v1:0",
                 region="us-east-1"):
        assert model_name.startswith("deepseek.") or model_name.startswith("us.deepseek.")
        super().__init__(model_name, region)

    def respond(self, messages, max_tokens, max_context_size):
        # deepseek does not support messages
        # from transformers import AutoTokenizer
        # tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1")
        # tokenizer.decode(tokenizer.apply_chat_template([{"role": "user", "content": "query1"}, {"role": "assistant", "content": "response1"}, {"role": "user", "content": "query2"}, ], tokenizer=False))
        assert isinstance(messages, list)
        # prompt = "<｜begin▁of▁sentence｜><｜User｜>query1<｜Assistant｜>response1<｜end▁of▁sentence｜><｜User｜>query2"
        # prompt = "<｜begin▁of▁sentence｜>"
        # for message in messages:
        #     if message["role"] == "system":
        #         prompt += "{}\n\n".format(message["content"])
        # if prompt.endswith("\n\n"):
        #     prompt = prompt[:-2]
        # for message in messages:
        #     if message["role"] == "user":
        #         prompt += "<｜User｜>{}".format(message["content"])
        #     elif message["role"] == "assistant":
        #         prompt += "<｜Assistant｜>{}".format(message["content"])
        # payload = {
        #     "prompt": prompt,
        #     "max_tokens": max_tokens,
        # }
        payload = {
            "messages": messages,
            "max_tokens": max_tokens
        }

        retry = 3
        while retry > 0:
            try:
                response = self.bedrock.invoke_model(body=json.dumps(payload),
                                                    modelId=self.model_name)
                return process_response(response)
            except Exception as e:
                retry -= 1
                if retry == 0:
                    raise e


class LlamaModel(BedRockModel):
    model_name_list = [
        "meta.llama2-13b-chat-v1:0:4k",
        "meta.llama2-13b-chat-v1",
        "meta.llama2-70b-chat-v1:0:4k",
        "meta.llama2-70b-chat-v1",
        "meta.llama2-13b-v1:0:4k",
        "meta.llama2-13b-v1",
        "meta.llama2-70b-v1:0:4k",
        "meta.llama2-70b-v1",
        "meta.llama3-8b-instruct-v1:0",
        "meta.llama3-70b-instruct-v1:0",
        "meta.llama3-1-70b-instruct-v1:0",
        "meta.llama3-2-90b-instruct-v1:0",
        "meta.llama3-3-70b-instruct-v1:0",
        "us.meta.llama3-1-70b-instruct-v1:0",
        "us.meta.llama3-2-90b-instruct-v1:0",
        "us.meta.llama3-3-70b-instruct-v1:0",
    ]

    def __init__(self, model_name="meta.llama3-8b-instruct-v1:0",
                 region="us-east-1"):
        assert model_name.startswith("meta.") or model_name.startswith("us.meta.")
        # assert model_name in LlamaModel.model_name_list
        super().__init__(model_name, region)

    def respond(self, messages, max_tokens, max_context_size):
        # lamma does not support messages
        # from transformers import AutoTokenizer
        # tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
        # tokenizer.decode(tokenizer.apply_chat_template([{"role": "user", "content": "query1"}, {"role": "assistant", "content": "response1"}, {"role": "user", "content": "query2"}, ], tokenizer=False))
        assert isinstance(messages, list)
        prompt = ""
        if self.model_name.startswith("meta.llama2") or self.model_name.startswith("us.meta.llama2"):
            for message in messages:
                if message["role"] == "user":
                    prompt += "<s> [INST] {} [/INST] ".format(message["content"])
                elif message["role"] == "system":
                    prompt += "<s> [INST] {} [/INST] Sure, I will follow the instruction. </s>".format(message["content"])
                elif message["role"] == "assistant":
                    prompt += "{} </s>".format(message["content"])
            prompt = prompt.strip()
        elif self.model_name.startswith("meta.llama3") or self.model_name.startswith("us.meta.llama3"):
            prompt = "<|begin_of_text|>"
            for message in messages:
                if message["role"] == "user":
                    prompt += "<|start_header_id|>user<|end_header_id|>\n\n{}<|eot_id|>".format(message["content"])
                elif message["role"] == "system":
                    prompt += "<|start_header_id|>system<|end_header_id|>\n\n{}<|eot_id|>".format(message["content"])
                elif message["role"] == "assistant":
                    prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n{}<|eot_id|>".format(message["content"])
        else:
            raise NotImplementedError

        payload = {
            "prompt": prompt,
            "max_gen_len": max_tokens,
        }
        prompt = prompt[-max_context_size:]
        
        retry = 3
        while retry > 0:
            try:
                response = self.bedrock.invoke_model(body=json.dumps(payload), modelId=self.model_name)
                return process_response(response)
            except Exception as e:
                retry -= 1
                if retry == 0:
                    raise e


class MistralModel(BedRockModel):
    model_name_list = [
        "mistral.mistral-7b-instruct-v0:2",
        "mistral.mixtral-8x7b-instruct-v0:1",
        "mistral.mistral-small-2402-v1:0",
        "mistral.mistral-large-2402-v1:0",
        "mistral.mistral-large-2407-v1:0",
        "mistral.mistral-large-2407-v1:0",
    ]

    def __init__(self, model_name="mistral.mistral-7b-instruct-v0:2",
                 region="us-east-1"):
        assert model_name.startswith("mistral.")
        # assert model_name in MistralModel.model_name_list
        super().__init__(model_name, region)

    def respond(self, messages, max_tokens, max_context_size):
        # mistral does not support messages
        # from transformers import AutoTokenizer
        # tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
        # tokenizer.decode(tokenizer.apply_chat_template([{"role": "user", "content": "query1"}, {"role": "assistant", "content": "response1"}, {"role": "user", "content": "query2"}, ], tokenizer=False))
        assert isinstance(messages, list)
        prompt = ""
        for message in messages:
            if message["role"] == "user":
                prompt += "<s> [INST] {} [/INST] ".format(message["content"])
            elif message["role"] == "system":
                prompt += "<s> [INST] {} [/INST] Sure, I will follow the instruction. </s>".format(message["content"])
            elif message["role"] == "assistant":
                prompt += "{} </s>".format(message["content"])
        prompt = prompt[-max_context_size:]
        prompt += "</s>"
        payload = {
            "prompt": prompt,
            "max_tokens": max_tokens,
        }

        retry = 3
        while retry > 0:
            try:
                response = self.bedrock.invoke_model(body=json.dumps(payload), modelId=self.model_name)
                return process_response(response)
            except Exception as e:
                retry -= 1
                if retry == 0:
                    raise e

class OpenAIModel(ModelBase):

    model_name_list = [
        "o1-preview", "o1-mini", "o4-mini",
        "gpt-4o", "gpt-4", "gpt-4o-mini", "gpt-4-turbo",
        "gpt-3.5-turbo", "gpt-3.5-turbo-instruct", "gpt-3.5-turbo-0125"
    ]

    def __init__(self, model_name, api_key):
        self.model_name = model_name
        self.client = openai.OpenAI(api_key=api_key)

    def respond(self, messages, max_tokens, max_context_size):

        retry = 3
        while retry > 0:
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    max_completion_tokens=max_tokens
                )
                return response.choices[0].message.content
            except Exception as e:
                retry -= 1
                if retry == 0:
                    raise e

def get_model(model_name, openai_key, region):
    if model_name in OpenAIModel.model_name_list:
        return OpenAIModel(model_name, openai_key)
    elif model_name.startswith("anthropic.") or model_name.startswith("us.anthropic."):
        return AnthropicModel(model_name, region)
    # elif model_name in LlamaModel.model_name_list:
    elif model_name.startswith("meta.") or model_name.startswith("us.meta."):
        return LlamaModel(model_name, region)
    # elif model_name in MistralModel.model_name_list:
    elif model_name.startswith("mistral.") or model_name.startswith("us.mistral."):
        return MistralModel(model_name, region)
    elif model_name.startswith("deepseek.") or model_name.startswith("us.deepseek."):
        return DeepseekModel(model_name, region)
    else:
        raise ValueError(f"Unknown model: {model_name}")



def get_dataset(dataset_file):
    data = []
    with open(dataset_file) as f:
        for line in f:
            data.append(json.loads(line))
    return data


def parse_args():
    def none_or_int(value):
        if value == 'None':
            return None
        return int(value)

    parser = argparse.ArgumentParser()
    # model
    parser.add_argument('--model', type=str, default='anthropic.claude-3-5-sonnet-20240620-v1:0')
    parser.add_argument('--openai-key', type=str, default='bedrock-api-key-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')
    parser.add_argument('--region', type=str, default="us-east-1")
    parser.add_argument('--use-ground-truth-context', action='store_true')
    # data
    parser.add_argument('--dataset-file', type=str, required=False)
    # evaluation
    parser.add_argument('--max-context-size', type=none_or_int, default=4096)
    parser.add_argument('--max-tokens-per-turn', type=int, default=512)
    # output
    parser.add_argument('--output-file', type=str, default='output.json')
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = parse_args()
    model = get_model(args.model, args.openai_key, args.region)
    # dataset = get_dataset(args.dataset_file)
    content = 'You are browsing an online shop. Based on the instruction, buy a product that close to the production description. You need to search, read the search results, pick a product, choose the size and color and buy. You should only choose action from the available actions list provided later.  Example process: I need a gingko light and 20x20 pillow cover that is hand painted. First search[gingko light 20x20 pillow cover hand painted], answer format: <answer>search[blanket with fleece throw]</answer>. Valid answer is search[<keywords>] or click[<clickable>].'
    # content += "Instruction: Find me machine wash men's dress shirts with polyester heathers, heathers cotton, cotton heather, needle sleeve, classic fit with color: baby blue, and fit type: youth, and size: large, and price lower than 50.00 dollars.We must buy a product within 10 actions. It doesn't have to match perfectly with description. Search term should not include details like size, color. Never search for more than 2 times.Do not be too strict about the description, it's more important to buy one that is close enough within action limit. Prioritize click a product in the current page over going to next page. Almost never click[next >] for more than 2 times.Almost never click[< prev] unless you are sure the product is on one of the previous pages. If you have less than 3 actions left, just buy the first product you see in the current page. If an matching option exists, make sure to click[size] then click[color], one at a time, before click[buy now], but don't have to if only 1 action left, in that case you just click[buy now]. Never click description. You must choose from these actions:search[<content>]. You have 9 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 200 words (tokens)."
    content += "Instruction: [SEP] Find me machine wash men's dress shirts with polyester heathers, heathers cotton, cotton heather, needle sleeve, classic fit with color: baby blue, and fit type: youth, and size: large, and price lower than 50.00 dollars [SEP] Back to Search [SEP] Page 1 (Total results: 38) [SEP] Next > [SEP] B0836D6CW4 [SEP] Sunbeam Heated Electric Fleece Throw Comforter Blanket with Controller, Auto Off Setting, Thermofine Wiring, and 3 Heat Settings, Red Plaid [SEP] $45.99 [SEP] B08FGWZZ8J [SEP] Itachishop Flannel Fleece Blanket with Pompom Fringe, Fuzzy Throw Blanket Bed Blanket for Couch Home Decor, 60x50in [SEP] $28.8 [SEP] B09DG3YTHY [SEP] Gogobebe Teal Green and Brown Flannel Fleece Throw Blanket for Sofa Couch Bed Retro Rustic Wood Grain Soft Cozy Lightweight Blanket for Adults/Kids 39x49inch [SEP] $31.39 [SEP] B09KTB1VG6 [SEP] FOHOG Collection Flannel Fleece Silky Soft Throw Shaggy Blanket Lightweight Comfy and Cozy Plush Microfiber Travel Silk 50 X 60 for Sofa Couch Bed (127 cm X 152 cm) (Buffalo Plaid) [SEP] $11.99 [SEP] B08SKH3LTM [SEP] RACHEL Rachel Roy Jacquard Textured Oversized Throw - Silky Soft and Cozy Flannel Fleece, Blanket for Bed and Couch - Oversized Throw 60 X 70, Coconut Milk [SEP] $26.99 [SEP] B09C1YWG8W [SEP] Soft Plush Electric Heated Blanket Throw with Foot Pocket | Navy Blue 50 x 62 | 3 Heat Settings with 2 Hour Auto Shut Off, UL Certified | Machine Washable [SEP] $69.99 [SEP] B08L2ZDWN2 [SEP] PAVILIA Decorative Sherpa Throw Pillow Covers, Set of 2, 18x18, Light Pink Blush Fluffy Pillow Cases for Couch, Bed, Sofa|Soft Accent Cushion Cover, Shaggy Living Room Decor [SEP] $13.99 [SEP] B09MSZ4VS8 [SEP] Goldweather Women Winter Leggings Printed Warm Fleece Lined Yoga Pants Trousers Thermal High Waist Thicken Cashmere Tights (Navy, L) [SEP] $20.99 [SEP] B08K7LDM7Q [SEP] 2 Pcs Cowhide Throw Pillow Covers Decorative Pillow Cases Farm Animal Brown Cow Hide Skin Print Pillow Case 18 X 18 Inch Velvet Square Cushion Cover for Sofa Bedroom [SEP] $17.99 [SEP] B09HX5CD2D [SEP] CSU Cleveland State University Vikings Property Fleece Drawstring Shorts Heather Charcoal [SEP] $39.95.We must buy a product within 10 actions. It doesn't have to match perfectly with description. Search term should not include details like size, color. Never search for more than 2 times. Do not be too strict about the description, it's more important to buy one that is close enough within action limit. Prioritize click a product in the current page over going to next page. Almost never click[next >] for more than 2 times.Almost never click[< prev] unless you are sure the product is on one of the previous pages. If you have less than 3 actions left, just buy the first product you see in the current page. If an matching option exists, make sure to click[size] then click[color], one at a time, before click[buy now], but don't have to if only 1 action left, in that case you just click[buy now]. Never click description. You must choose from these actions:click[back to search], click[next >], click[b0836d6cw4], click[b08fgwzz8j], click[b09dg3ythy], click[b09ktb1vg6], click[b08skh3ltm], click[b09c1ywg8w], click[b08l2zdwn2], click[b09msz4vs8], click[b08k7ldm7q], click[b09hx5cd2d]. You have 8 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 200 words (tokens)."
    dataset = [{"id": "sample-1", "messages": [
                {"role": "user", "content": content}
                ]}]
    
    used_ids = set()

    if os.path.exists(args.output_file):
        with open(args.output_file, "r") as f:
            for line in f:
                x = json.loads(line)
                used_ids.add(x["id"])

    with open(args.output_file, "a") as f:
        for x in dataset:
            if x["id"] in used_ids:
                continue
            response = model.respond(
                x["messages"],
                max_tokens=args.max_tokens_per_turn,
                max_context_size=args.max_context_size
            )
            x["response"] = response
            used_ids.add(x["id"])
            f.write(json.dumps(x))
            f.write("\n")




# import boto3
# from botocore.exceptions import ClientError
# import os
# from collections import defaultdict


# def list_foundation_models():
#     bedrock_client = boto3.client(service_name="bedrock", region_name="us-east-1")
#     response = bedrock_client.list_foundation_models()['modelSummaries']
#     model_dict = defaultdict(list)
#     for model_item in response:
#         model_dict[model_item['providerName']].append([model_item['modelName'], model_item['modelId']])
#     for provider, models in model_dict.items():
#         print(f"Provider: {provider}")
#         models = sorted(models, key=lambda x: x[0])
#         print(f"Models: {models}")
#         print()


# def call_LLM():
#     brt = boto3.client("bedrock-runtime", region_name="us-east-1")    # Create an Amazon Bedrock Runtime client.
    
#     # Model ID. Don't forget the "us." at the beginning!
#     # model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"   # Claude Sonnet 3.7
#     model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"   # Claude Sonnet 4
#     # model_id = "us.anthropic.claude-opus-4-20250514-v1:0"   # Claude Opus 4

#     # Start a conversation with the user message.
#     user_message = "Describe the purpose of a 'hello world' program in one line."
#     conversation = [
#         {
#             "role": "user",
#             "content": [{"text": user_message}],
#         }
#     ]

#     try:
#         response = brt.converse(
#             modelId=model_id,
#             system=[{"text": "You are a helpful assistant."}],
#             messages=conversation,
#             inferenceConfig={"maxTokens": 512, "temperature": 0.5, "topP": 0.9},
#             additionalModelRequestFields={"top_k": 200}
#         )
#         assert response['stopReason'] == "end_turn"   # Ensure that the response is complete.
#         response_text = response["output"]["message"]["content"][0]["text"]
#         print(response_text)

#     except (ClientError, Exception) as e:
#         print(f"ERROR: Can't invoke '{model_id}'. Reason: {e}")
#         exit(1)
    

# if __name__ == '__main__':
#     # list_foundation_models()
#     call_LLM()


