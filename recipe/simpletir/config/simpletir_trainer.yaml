hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  val_sample_size: null
  apply_chat_template: True
  filter_overlong_prompts: True
  max_start_length: 4096
  max_obs_length: 256
  prompt: |
    Solve the following problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after "Code execution result: ") is returned to aid your reasoning and help you arrive at the final answer. The Python code should be complete scripts, including necessary imports.

    Code Format:
    Each code snippet is wrapped between ```. You need to use `print()` to output intermediate results.

    Answer Format:
    You can use the `final_answer()` function in the code to return your final answer. For example, to answer the User Question: What is the result of the 5 + 3 + 1294.678?, you can write:
    ```py
    answer = 5 + 3 + 1294.678
    final_answer(answer)
    ```

    You can also use \boxed to return your answer. The last part of your response should be:
    \boxed{'The final answer goes here.'}

    User Question:

actor_rollout_ref:
  actor:
    optim:
      lr: 1e-6
    use_dynamic_bsz: True
    use_kl_loss: False
    kl_loss_coef: 0.0
    kl_loss_type: low_var_kl
    entropy_coeff: 0.0
    clip_ratio_high: 0.28
    clip_ratio_low: 0.2
    clip_ratio_c: 3.0
    mask_tool_output: True
    mask_void_turns: True
  model:
    use_remove_padding: True
  rollout:
    temperature: 1.0
    min_p: 0.0
    swap_space: 40
    model_path: ${actor_rollout_ref.model.path}
    disable_log_stats: False
    enforce_eager: False
    free_cache_engine: False
    n: 16
    min_n: 4 # at least 4 responses should be valid for each prompt, otherwise we will skip it. Invalid ones include those which are over long.
    val_kwargs:
      # sampling parameters for validation
      top_k: -1 # 0 for hf rollout, -1 for vllm rollout
      top_p: 0.7
      temperature: 1.0
      n: 4
      do_sample: True # default eager for validation
      k: ${actor_rollout_ref.rollout.val_kwargs.n} # pass@k, 1 <= k_val <= actor_rollout_ref.rollout.n_val, default actor_rollout_ref.rollout.n_val
    ref:
      fsdp_config:
        param_offload: True

trainer:
  rejection_sample: True
  oversample_multiplier: 2.0 # Multiple the training batch size by this factor to account for rejection sampling reduction.
  remove_clip: False # remove overlong response if True
  acc_filter: False # if True, only keep prompts with avg acc ratio in thresholds
  acc_filter_high: 1
  acc_filter_low: 0
  start_clip_step: 50
  critic_warmup: 0
  logger: ['console','wandb']
  project_name: simpletir
  experiment_name: simpletir_math

critic:
  ppo_micro_batch_size_per_gpu: 4

reward_model:
  reward_manager: math

algorithm:
  use_kl_in_reward: False
  kl_ctrl:
    kl_coef: 0.0

agent:
  tool_use: True
  max_turns: 5
  append_final_answer_func: True
